---
import DocsLayout from '@/layouts/DocsLayout.astro'
import { Icon } from 'astro-icon/components'
---

<DocsLayout title="Groq LLM Setup">
  <h1>Groq LLM Integration</h1>
  <p class="text-xl text-foreground/80 mt-2">
    Configure ClawChan to use Groq's ultra-fast LLM inference powered by their custom LPU (Language Processing Unit) technology.
  </p>

  <div class="callout callout-success mt-6">
    <div class="callout-title">
      <Icon name="lucide:zap" class="size-5" />
      Why Groq?
    </div>
    <p class="text-sm">
      Groq provides **lightning-fast inference** with response times under 1 second, making it perfect for real-time chat applications and autonomous agents like ClawChan. It's also cost-effective with a generous free tier.
    </p>
  </div>

  ## What is Groq?

  Groq is an AI infrastructure company that has built a revolutionary **Language Processing Unit (LPU)** - custom silicon specifically designed for LLM inference. Unlike traditional GPUs, Groq's LPU delivers:

  - ‚ö° **Ultra-fast inference**: Sub-second response times
  - üß† **High-quality models**: Access to Llama 3.1, Mixtral, Gemma
  - üí∞ **Cost-effective**: Competitive pricing with free tier
  - üîÑ **OpenAI-compatible API**: Easy integration
  - üéØ **Consistent performance**: Low latency even under load

  ## Performance Comparison

  <div class="bento-card p-6 my-6">
    <table class="w-full">
      <thead>
        <tr>
          <th>Provider</th>
          <th>Model</th>
          <th>Avg Response Time</th>
          <th>Cost (per 1M tokens)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong class="text-cyan-glow">Groq</strong></td>
          <td>Llama 3.1 70B</td>
          <td class="text-green-500 font-bold">&lt;1 second</td>
          <td>$0.59 / $0.79</td>
        </tr>
        <tr>
          <td>OpenAI</td>
          <td>GPT-4</td>
          <td>5-10 seconds</td>
          <td>$30.00 / $60.00</td>
        </tr>
        <tr>
          <td>Anthropic</td>
          <td>Claude 3</td>
          <td>3-7 seconds</td>
          <td>$3.00 / $15.00</td>
        </tr>
        <tr>
          <td>Together AI</td>
          <td>Llama 3.1 70B</td>
          <td>2-4 seconds</td>
          <td>$0.88 / $0.88</td>
        </tr>
      </tbody>
    </table>
  </div>

  ## Step 1: Create Groq Account

  1. Visit [https://console.groq.com](https://console.groq.com)
  2. Click **Sign Up** or **Get Started**
  3. Sign up with:
     - Email address
     - Google account
     - GitHub account

  <div class="callout callout-info">
    <div class="callout-title">
      <Icon name="lucide:gift" class="size-5" />
      Free Tier Benefits
    </div>
    <p class="text-sm mb-2">Groq's free tier includes:</p>
    <ul class="text-sm space-y-1">
      <li>30 requests per minute</li>
      <li>14,400 requests per day (more than enough for development)</li>
      <li>Access to all models (Llama, Mixtral, Gemma)</li>
      <li>No credit card required</li>
    </ul>
  </div>

  ## Step 2: Generate API Key

  ### Via Console UI

  1. Log in to [https://console.groq.com](https://console.groq.com)
  2. Navigate to **API Keys** in the left sidebar
  3. Click **Create API Key**
  4. Enter a name for your key:
     - For development: `ClawChan-Dev`
     - For production: `ClawChan-Production`
  5. Click **Create**
  6. **Copy the API key immediately** - it won't be shown again!

  <div class="callout callout-danger">
    <div class="callout-title">
      <Icon name="lucide:shield-alert" class="size-5" />
      Security Critical
    </div>
    <p class="text-sm">
      Store your API key securely! Never commit it to Git, share it publicly, or expose it in client-side code. If your key is compromised, delete it immediately from the Groq console and generate a new one.
    </p>
  </div>

  ## Step 3: Configure Environment Variables

  Add your Groq API key to the `.env` file:

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">.env</span>
      <button class="code-block-copy">Copy</button>
    </div>
    <pre><code># ============================================
# Groq LLM Configuration
# ============================================
GROQ_API_KEY=gsk_your_actual_api_key_here
LLM_API_KEY=gsk_your_actual_api_key_here

# Model Provider
MODEL_PROVIDER=groq

# Groq Model Selection
GROQ_MODEL=llama-3.1-70b-versatile

# Groq API Base URL (don't change unless using proxy)
GROQ_API_BASE_URL=https://api.groq.com/openai/v1</code></pre>
  </div>

  ### Environment Variable Explanation

  | Variable | Description | Example |
  |----------|-------------|---------|
  | `GROQ_API_KEY` | Your Groq API key from console | `gsk_abc123...` |
  | `LLM_API_KEY` | Fallback LLM key (same as GROQ) | `gsk_abc123...` |
  | `MODEL_PROVIDER` | Tell ElizaOS to use Groq | `groq` |
  | `GROQ_MODEL` | Specific model to use | `llama-3.1-70b-versatile` |
  | `GROQ_API_BASE_URL` | Groq API endpoint | `https://api.groq.com/openai/v1` |

  ## Step 4: Choose Your Model

  Groq provides several high-performance models. Choose based on your needs:

  ### Llama 3.1 70B Versatile (Recommended)

  <div class="bento-card p-5 my-4 border-cyan-glow/30">
    <div class="flex items-start gap-4">
      <div class="flex-1">
        <h4 class="mb-2">Best for: Production ClawChan deployment</h4>
        <ul class="text-sm space-y-1">
          <li>‚úÖ 70 billion parameters - excellent reasoning</li>
          <li>‚úÖ Large context window (128k tokens)</li>
          <li>‚úÖ Great balance of speed and quality</li>
          <li>‚úÖ Versatile for all tasks</li>
          <li>‚úÖ Response time: ~800ms</li>
        </ul>
      </div>
    </div>
    <div class="code-block mt-4">
      <div class="code-block-header">
        <span class="code-block-lang">Configuration</span>
        <button class="code-block-copy">Copy</button>
      </div>
      <pre><code>GROQ_MODEL=llama-3.1-70b-versatile</code></pre>
    </div>
  </div>

  ### Llama 3.1 8B Instant (Fast)

  <div class="bento-card p-5 my-4 border-amber/30">
    <div class="flex items-start gap-4">
      <div class="flex-1">
        <h4 class="mb-2">Best for: Development and simple queries</h4>
        <ul class="text-sm space-y-1">
          <li>‚úÖ 8 billion parameters - fast inference</li>
          <li>‚úÖ Response time: ~300ms</li>
          <li>‚úÖ Good for simple questions</li>
          <li>‚ö†Ô∏è Less capable for complex reasoning</li>
        </ul>
      </div>
    </div>
    <div class="code-block mt-4">
      <div class="code-block-header">
        <span class="code-block-lang">Configuration</span>
        <button class="code-block-copy">Copy</button>
      </div>
      <pre><code>GROQ_MODEL=llama-3.1-8b-instant</code></pre>
    </div>
  </div>

  ### Mixtral 8x7B (Alternative)

  <div class="bento-card p-5 my-4">
    <div class="flex items-start gap-4">
      <div class="flex-1">
        <h4 class="mb-2">Best for: Complex reasoning tasks</h4>
        <ul class="text-sm space-y-1">
          <li>‚úÖ Mixture of Experts architecture</li>
          <li>‚úÖ Excellent for technical/coding tasks</li>
          <li>‚úÖ 32k context window</li>
          <li>‚úÖ Response time: ~1s</li>
        </ul>
      </div>
    </div>
    <div class="code-block mt-4">
      <div class="code-block-header">
        <span class="code-block-lang">Configuration</span>
        <button class="code-block-copy">Copy</button>
      </div>
      <pre><code>GROQ_MODEL=mixtral-8x7b-32768</code></pre>
    </div>
  </div>

  ### Model Comparison Table

  <div class="bento-card p-6 my-6 overflow-x-auto">
    <table class="w-full">
      <thead>
        <tr>
          <th>Model</th>
          <th>Parameters</th>
          <th>Context Window</th>
          <th>Speed</th>
          <th>Use Case</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>llama-3.1-70b-versatile</code></td>
          <td>70B</td>
          <td>128k</td>
          <td>~800ms</td>
          <td>Production, all-purpose</td>
        </tr>
        <tr>
          <td><code>llama-3.1-8b-instant</code></td>
          <td>8B</td>
          <td>128k</td>
          <td>~300ms</td>
          <td>Dev, simple queries</td>
        </tr>
        <tr>
          <td><code>mixtral-8x7b-32768</code></td>
          <td>8x7B (MoE)</td>
          <td>32k</td>
          <td>~1s</td>
          <td>Complex reasoning</td>
        </tr>
        <tr>
          <td><code>gemma-7b-it</code></td>
          <td>7B</td>
          <td>8k</td>
          <td>~400ms</td>
          <td>Lightweight tasks</td>
        </tr>
      </tbody>
    </table>
  </div>

  ## Step 5: Verify Configuration

  ### Test Connection

  Create a test script to verify your Groq setup:

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">JavaScript - test-groq.mjs</span>
      <button class="code-block-copy">Copy</button>
    </div>
    <pre><code>import fetch from 'node-fetch'
import dotenv from 'dotenv'

dotenv.config()

const GROQ_API_KEY = process.env.GROQ_API_KEY
const GROQ_MODEL = process.env.GROQ_MODEL || 'llama-3.1-70b-versatile'

async function testGroq() {
  console.log('üß™ Testing Groq API connection...')
  console.log(`üìä Model: ${GROQ_MODEL}`)

  const startTime = Date.now()

  try {
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${GROQ_API_KEY}`
      },
      body: JSON.stringify({
        model: GROQ_MODEL,
        messages: [
          {
            role: 'user',
            content: 'Say "ClawChan is online!" in one sentence.'
          }
        ],
        temperature: 0.7,
        max_tokens: 50
      })
    })

    const data = await response.json()
    const endTime = Date.now()
    const latency = endTime - startTime

    if (data.choices && data.choices[0]) {
      console.log('‚úÖ Groq API: Connected')
      console.log(`‚ö° Response time: ${latency}ms`)
      console.log(`üí¨ Response: ${data.choices[0].message.content}`)
      console.log(`üî¢ Tokens used: ${data.usage.total_tokens}`)
    } else {
      console.error('‚ùå Unexpected response:', data)
    }
  } catch (error) {
    console.error('‚ùå Connection failed:', error.message)
  }
}

testGroq()</code></pre>
  </div>

  Run the test:

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">Bash</span>
      <button class="code-block-copy">Copy</button>
    </div>
    <pre><code>node test-groq.mjs</code></pre>
  </div>

  **Expected output:**
  ```
  üß™ Testing Groq API connection...
  üìä Model: llama-3.1-70b-versatile
  ‚úÖ Groq API: Connected
  ‚ö° Response time: 847ms
  üí¨ Response: ClawChan is online!
  üî¢ Tokens used: 23
  ```

  ### Start Agent with Groq

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">Bash</span>
      <button class="code-block-copy">Copy</button>
    </div>
    <pre><code># Start agent
npm run agent

# Or in development mode with auto-restart
npm run agent:dev</code></pre>
  </div>

  **Expected output:**

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">Agent Logs</span>
    </div>
    <pre><code>‚úÖ Character: ClawChan
‚úÖ LLM Provider: GROQ
‚úÖ API Key: gsk_YUsm...

üîß Configuration:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìõ Agent Name: ClawChan
ü§ñ Autonomous Mode: true
üß† LLM Provider: GROQ
üìä Model: llama-3.1-70b-versatile
ü¶û Moltbook Token: ‚úÖ Set
üîë LLM API Key: ‚úÖ Set
‚ö° Groq Base URL: https://api.groq.com/openai/v1
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üöÄ Using Groq with model: llama-3.1-70b-versatile
‚ú® ClawChan Agent is now running with Moltbook integration!</code></pre>
  </div>

  ## Advanced Configuration

  ### Custom Model Parameters

  You can customize Groq model behavior in `agent/index.ts`:

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">TypeScript - agent/index.ts</span>
      <button class="code-block-copy">Copy</button>
    </div>
    <pre><code>character.settings = {
  ...character.settings,
  model: {
    provider: 'groq',
    name: process.env.GROQ_MODEL || 'llama-3.1-70b-versatile',
    apiKey: llmApiKey,
    baseURL: process.env.GROQ_API_BASE_URL,

    // Advanced parameters
    temperature: 0.7,        // Creativity (0.0-1.0)
    maxTokens: 1000,         // Max response length
    topP: 0.9,               // Nucleus sampling
    frequencyPenalty: 0.0,   // Reduce repetition
    presencePenalty: 0.0,    // Encourage new topics
  }
}</code></pre>
  </div>

  ### Rate Limit Handling

  Implement exponential backoff for rate limits:

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">TypeScript</span>
      <button class="code-block-copy">Copy</button>
    </div>
    <pre><code>async function chatWithRetry(message: string, maxRetries = 3) {
  for (let i = 0; i < maxRetries; i++) {
    try {
      return await groq.chat(message)
    } catch (error) {
      if (error.code === 'RATE_LIMIT_EXCEEDED' && i < maxRetries - 1) {
        const delay = Math.pow(2, i) * 1000 // Exponential backoff
        console.log(`‚è≥ Rate limited. Retrying in ${delay}ms...`)
        await new Promise(resolve => setTimeout(resolve, delay))
      } else {
        throw error
      }
    }
  }
}</code></pre>
  </div>

  ## Monitoring and Optimization

  ### Monitor API Usage

  Track your Groq API usage at [https://console.groq.com/usage](https://console.groq.com/usage):

  - **Requests per minute** - Monitor rate limits
  - **Tokens consumed** - Track costs
  - **Latency metrics** - Performance insights
  - **Error rates** - Debug issues

  ### Optimization Tips

  1. **Use the right model**:
     - Development: `llama-3.1-8b-instant` (fastest)
     - Production: `llama-3.1-70b-versatile` (best quality)

  2. **Optimize prompts**:
     - Keep system prompts concise
     - Use clear, specific instructions
     - Avoid unnecessary context

  3. **Cache responses**:
     - Cache common queries
     - Implement response deduplication

  4. **Batch requests**:
     - Group similar queries
     - Use async/await for parallel requests

  ## Cost Estimation

  ### Free Tier

  <div class="bento-card p-5">
    <h4 class="text-warm-gold mb-3">Free Tier Limits</h4>
    <ul class="space-y-2 text-sm">
      <li>**30 requests/minute** - ~43,200 requests/day (if sustained)</li>
      <li>**14,400 requests/day** - Official daily limit</li>
      <li>**All models included** - No model restrictions</li>
      <li>**No expiration** - Free tier doesn't expire</li>
    </ul>
  </div>

  ### Paid Plans

  If you need more, Groq offers paid plans:

  | Plan | Requests/Min | Requests/Day | Price |
  |------|-------------|--------------|-------|
  | **Free** | 30 | 14,400 | $0 |
  | **Developer** | 60 | 28,800 | ~$10/month |
  | **Business** | 600 | 288,000 | Custom pricing |

  Contact Groq sales for enterprise pricing.

  ## Troubleshooting

  ### "Invalid API Key" Error

  **Problem**: `Error: Invalid API key`

  **Solutions**:
  1. Verify key format starts with `gsk_`
  2. Check for extra spaces in `.env` file
  3. Ensure quotes are removed: `GROQ_API_KEY=gsk_...` (not `"gsk_..."`)
  4. Generate new key at [console.groq.com](https://console.groq.com)

  ### "Rate Limit Exceeded" Error

  **Problem**: `Error: Rate limit exceeded`

  **Solutions**:
  1. Check current usage at [console.groq.com/usage](https://console.groq.com/usage)
  2. Implement exponential backoff (see Advanced Configuration)
  3. Upgrade to paid plan if consistently hitting limits
  4. Reduce request frequency in autonomous mode:
     ```
     MOLTBOOK_AUTONOMY_MAX_STEPS=5  # Reduce from default
     ```

  ### Slow Response Times

  **Problem**: Responses taking >2 seconds

  **Solutions**:
  1. Switch to faster model: `GROQ_MODEL=llama-3.1-8b-instant`
  2. Reduce max_tokens limit
  3. Check network latency with ping test
  4. Verify Groq status at [status.groq.com](https://status.groq.com)

  ### Model Not Found

  **Problem**: `Error: Model not found`

  **Solutions**:
  1. Check available models at [console.groq.com/docs/models](https://console.groq.com/docs/models)
  2. Verify spelling in `.env`:
     ```
     GROQ_MODEL=llama-3.1-70b-versatile  # Correct
     GROQ_MODEL=llama-3.1-70b            # Wrong
     ```

  ## Migration from Other Providers

  ### From OpenAI

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">Before (OpenAI)</span>
    </div>
    <pre><code>MODEL_PROVIDER=openai
OPENAI_API_KEY=sk_...
OPENAI_MODEL=gpt-4</code></pre>
  </div>

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">After (Groq)</span>
    </div>
    <pre><code>MODEL_PROVIDER=groq
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.1-70b-versatile</code></pre>
  </div>

  ### From Anthropic (Claude)

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">Before (Anthropic)</span>
    </div>
    <pre><code>MODEL_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-opus</code></pre>
  </div>

  <div class="code-block">
    <div class="code-block-header">
      <span class="code-block-lang">After (Groq)</span>
    </div>
    <pre><code>MODEL_PROVIDER=groq
GROQ_API_KEY=gsk_...
GROQ_MODEL=llama-3.1-70b-versatile</code></pre>
  </div>

  ## Next Steps

  <div class="grid gap-4 sm:grid-cols-2 my-8">
    <a href="/docs/moltbook-integration" class="bento-card group p-5 hover:border-warm-gold/50 transition-colors">
      <div class="flex items-center gap-2 mb-3">
        <Icon name="lucide:book-open" class="text-warm-gold size-5" />
        <h3 class="text-base font-semibold mb-0">Moltbook Integration</h3>
      </div>
      <p class="text-sm text-muted-foreground">Connect ClawChan to Moltbook social platform.</p>
    </a>

    <a href="/docs/agent-development" class="bento-card group p-5 hover:border-cyan-glow/50 transition-colors">
      <div class="flex items-center gap-2 mb-3">
        <Icon name="lucide:code-2" class="text-cyan-glow size-5" />
        <h3 class="text-base font-semibold mb-0">Agent Development</h3>
      </div>
      <p class="text-sm text-muted-foreground">Customize ClawChan's behavior and responses.</p>
    </a>
  </div>

  <div class="callout callout-success">
    <div class="callout-title">
      <Icon name="lucide:zap" class="size-5" />
      Groq Setup Complete!
    </div>
    <p class="text-sm">
      ClawChan is now powered by Groq's ultra-fast LLM inference! You should experience sub-second response times for most queries. Test it at <a href="https://www.elizacloud.ai/dashboard/chat?characterId=e6a4f6c4-ef1a-40c3-87c3-9305d6dcc5c1" target="_blank" rel="noopener noreferrer">/chat</a> and see the speed difference.
    </p>
  </div>
</DocsLayout>
